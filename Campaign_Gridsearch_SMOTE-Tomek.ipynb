{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import make_scorer, classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, cohen_kappa_score \n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.pipeline import make_pipeline, Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "from imblearn.combine import SMOTETomek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = pd.read_csv('df_data.csv')\n",
    "df = pd.DataFrame(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 41176 entries, 0 to 41175\n",
      "Data columns (total 27 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   euribor3m          41176 non-null  float64\n",
      " 1   cons.conf.idx      41176 non-null  float64\n",
      " 2   age                41176 non-null  float64\n",
      " 3   previous           41176 non-null  float64\n",
      " 4   blue_collar        41176 non-null  int64  \n",
      " 5   student            41176 non-null  int64  \n",
      " 6   retiree            41176 non-null  int64  \n",
      " 7   unemployed         41176 non-null  int64  \n",
      " 8   single             41176 non-null  int64  \n",
      " 9   no_default         41176 non-null  int64  \n",
      " 10  age_student        41176 non-null  float64\n",
      " 11  age_retiree        41176 non-null  float64\n",
      " 12  contact_telephone  41176 non-null  int64  \n",
      " 13  month_aug          41176 non-null  int64  \n",
      " 14  month_dec          41176 non-null  int64  \n",
      " 15  month_jul          41176 non-null  int64  \n",
      " 16  month_jun          41176 non-null  int64  \n",
      " 17  month_mar          41176 non-null  int64  \n",
      " 18  month_may          41176 non-null  int64  \n",
      " 19  month_nov          41176 non-null  int64  \n",
      " 20  month_oct          41176 non-null  int64  \n",
      " 21  month_sep          41176 non-null  int64  \n",
      " 22  day_of_week_mon    41176 non-null  int64  \n",
      " 23  day_of_week_thu    41176 non-null  int64  \n",
      " 24  day_of_week_tue    41176 non-null  int64  \n",
      " 25  day_of_week_wed    41176 non-null  int64  \n",
      " 26  target             41176 non-null  int64  \n",
      "dtypes: float64(6), int64(21)\n",
      "memory usage: 8.5 MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data before us is very imbalanced, with only 12.7% of the positive class. The choice of scoring metric on which to train the machine learning algorithm is therefore extremely important. The typical accuracy scorer is not the best choice with such an imbalanced dataset because one could get an apparently high accuracy score of approximately 87% if the model just simply picks the negative class. This is obviously unhelpful to the goal of predicting the positive class.\n",
    "\n",
    "#### The F1 score tends to be the better metric for imbalanced datasets as it evaluates both the precision and recall rates, so it is focused on how good the model is at predicting the positive (minority in this case) class. Another good metric is Cohen's Kappa, which takes into account how much agreement would be expected by chance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_scorer = make_scorer(f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling the Explanatory Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df.pop('target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['euribor3m', 'cons.conf.idx', 'age', 'previous', 'blue_collar',\n",
       "       'student', 'retiree', 'unemployed', 'single', 'no_default',\n",
       "       'age_student', 'age_retiree', 'contact_telephone', 'month_aug',\n",
       "       'month_dec', 'month_jul', 'month_jun', 'month_mar', 'month_may',\n",
       "       'month_nov', 'month_oct', 'month_sep', 'day_of_week_mon',\n",
       "       'day_of_week_thu', 'day_of_week_tue', 'day_of_week_wed'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.3, random_state=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will use MinMax scaler to scale the variables because (1) I don't need normally distributed data because not using OLS, (2) plus this is a sizeable dataset, and (2) the prevalence of dummy variables among the explanatory variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "X_train_mm = scaler.fit_transform(X_train)\n",
    "X_test_mm = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.DataFrame(X_train_mm, columns=['euribor3m', 'cons.conf.idx', 'age', 'previous', 'blue_collar',\n",
    "       'student', 'retiree', 'unemployed', 'single', 'no_default', 'age_student', 'age_retiree', 'contact_telephone', \n",
    "        'month_aug', 'month_dec', 'month_jul', 'month_jun', 'month_mar', 'month_may', 'month_nov', 'month_oct', \n",
    "        'month_sep', 'day_of_week_mon', 'day_of_week_thu', 'day_of_week_tue', 'day_of_week_wed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.DataFrame(X_test_mm, columns=['euribor3m', 'cons.conf.idx', 'age', 'previous', 'blue_collar',\n",
    "       'student', 'retiree', 'unemployed', 'single', 'no_default', 'age_student', 'age_retiree', 'contact_telephone', \n",
    "        'month_aug', 'month_dec', 'month_jul', 'month_jun', 'month_mar', 'month_may', 'month_nov', 'month_oct', \n",
    "        'month_sep', 'day_of_week_mon', 'day_of_week_thu', 'day_of_week_tue', 'day_of_week_wed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>euribor3m</th>\n",
       "      <th>cons.conf.idx</th>\n",
       "      <th>age</th>\n",
       "      <th>previous</th>\n",
       "      <th>blue_collar</th>\n",
       "      <th>student</th>\n",
       "      <th>retiree</th>\n",
       "      <th>unemployed</th>\n",
       "      <th>single</th>\n",
       "      <th>no_default</th>\n",
       "      <th>...</th>\n",
       "      <th>month_jun</th>\n",
       "      <th>month_mar</th>\n",
       "      <th>month_may</th>\n",
       "      <th>month_nov</th>\n",
       "      <th>month_oct</th>\n",
       "      <th>month_sep</th>\n",
       "      <th>day_of_week_mon</th>\n",
       "      <th>day_of_week_thu</th>\n",
       "      <th>day_of_week_tue</th>\n",
       "      <th>day_of_week_wed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>28823.000000</td>\n",
       "      <td>28823.000000</td>\n",
       "      <td>28823.000000</td>\n",
       "      <td>28823.000000</td>\n",
       "      <td>28823.000000</td>\n",
       "      <td>28823.000000</td>\n",
       "      <td>28823.000000</td>\n",
       "      <td>28823.000000</td>\n",
       "      <td>28823.000000</td>\n",
       "      <td>28823.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>28823.000000</td>\n",
       "      <td>28823.000000</td>\n",
       "      <td>28823.00000</td>\n",
       "      <td>28823.000000</td>\n",
       "      <td>28823.000000</td>\n",
       "      <td>28823.000000</td>\n",
       "      <td>28823.000000</td>\n",
       "      <td>28823.000000</td>\n",
       "      <td>28823.000000</td>\n",
       "      <td>28823.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.677648</td>\n",
       "      <td>0.430109</td>\n",
       "      <td>0.283754</td>\n",
       "      <td>0.028796</td>\n",
       "      <td>0.222912</td>\n",
       "      <td>0.020747</td>\n",
       "      <td>0.040731</td>\n",
       "      <td>0.024217</td>\n",
       "      <td>0.281546</td>\n",
       "      <td>0.792561</td>\n",
       "      <td>...</td>\n",
       "      <td>0.129515</td>\n",
       "      <td>0.012906</td>\n",
       "      <td>0.33279</td>\n",
       "      <td>0.099886</td>\n",
       "      <td>0.017521</td>\n",
       "      <td>0.013600</td>\n",
       "      <td>0.207265</td>\n",
       "      <td>0.209728</td>\n",
       "      <td>0.196718</td>\n",
       "      <td>0.197516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.392972</td>\n",
       "      <td>0.193246</td>\n",
       "      <td>0.128394</td>\n",
       "      <td>0.082555</td>\n",
       "      <td>0.416207</td>\n",
       "      <td>0.142540</td>\n",
       "      <td>0.197671</td>\n",
       "      <td>0.153724</td>\n",
       "      <td>0.449761</td>\n",
       "      <td>0.405479</td>\n",
       "      <td>...</td>\n",
       "      <td>0.335774</td>\n",
       "      <td>0.112873</td>\n",
       "      <td>0.47122</td>\n",
       "      <td>0.299852</td>\n",
       "      <td>0.131203</td>\n",
       "      <td>0.115826</td>\n",
       "      <td>0.405354</td>\n",
       "      <td>0.407122</td>\n",
       "      <td>0.397524</td>\n",
       "      <td>0.398132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.160961</td>\n",
       "      <td>0.338912</td>\n",
       "      <td>0.185185</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.957379</td>\n",
       "      <td>0.376569</td>\n",
       "      <td>0.259259</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.980957</td>\n",
       "      <td>0.602510</td>\n",
       "      <td>0.370370</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          euribor3m  cons.conf.idx           age      previous   blue_collar  \\\n",
       "count  28823.000000   28823.000000  28823.000000  28823.000000  28823.000000   \n",
       "mean       0.677648       0.430109      0.283754      0.028796      0.222912   \n",
       "std        0.392972       0.193246      0.128394      0.082555      0.416207   \n",
       "min        0.000000       0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.160961       0.338912      0.185185      0.000000      0.000000   \n",
       "50%        0.957379       0.376569      0.259259      0.000000      0.000000   \n",
       "75%        0.980957       0.602510      0.370370      0.000000      0.000000   \n",
       "max        1.000000       1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "            student       retiree    unemployed        single    no_default  \\\n",
       "count  28823.000000  28823.000000  28823.000000  28823.000000  28823.000000   \n",
       "mean       0.020747      0.040731      0.024217      0.281546      0.792561   \n",
       "std        0.142540      0.197671      0.153724      0.449761      0.405479   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.000000      0.000000      0.000000      0.000000      1.000000   \n",
       "50%        0.000000      0.000000      0.000000      0.000000      1.000000   \n",
       "75%        0.000000      0.000000      0.000000      1.000000      1.000000   \n",
       "max        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "       ...     month_jun     month_mar    month_may     month_nov  \\\n",
       "count  ...  28823.000000  28823.000000  28823.00000  28823.000000   \n",
       "mean   ...      0.129515      0.012906      0.33279      0.099886   \n",
       "std    ...      0.335774      0.112873      0.47122      0.299852   \n",
       "min    ...      0.000000      0.000000      0.00000      0.000000   \n",
       "25%    ...      0.000000      0.000000      0.00000      0.000000   \n",
       "50%    ...      0.000000      0.000000      0.00000      0.000000   \n",
       "75%    ...      0.000000      0.000000      1.00000      0.000000   \n",
       "max    ...      1.000000      1.000000      1.00000      1.000000   \n",
       "\n",
       "          month_oct     month_sep  day_of_week_mon  day_of_week_thu  \\\n",
       "count  28823.000000  28823.000000     28823.000000     28823.000000   \n",
       "mean       0.017521      0.013600         0.207265         0.209728   \n",
       "std        0.131203      0.115826         0.405354         0.407122   \n",
       "min        0.000000      0.000000         0.000000         0.000000   \n",
       "25%        0.000000      0.000000         0.000000         0.000000   \n",
       "50%        0.000000      0.000000         0.000000         0.000000   \n",
       "75%        0.000000      0.000000         0.000000         0.000000   \n",
       "max        1.000000      1.000000         1.000000         1.000000   \n",
       "\n",
       "       day_of_week_tue  day_of_week_wed  \n",
       "count     28823.000000     28823.000000  \n",
       "mean          0.196718         0.197516  \n",
       "std           0.397524         0.398132  \n",
       "min           0.000000         0.000000  \n",
       "25%           0.000000         0.000000  \n",
       "50%           0.000000         0.000000  \n",
       "75%           0.000000         0.000000  \n",
       "max           1.000000         1.000000  \n",
       "\n",
       "[8 rows x 26 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Confirms that the max-min scaling has taken place\n",
    "X_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gridsearch Logit, Random Forest & SVC models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = StratifiedKFold(n_splits=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit = LogisticRegression(penalty='l2', C=1, solver='liblinear', class_weight='balanced', random_state=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_gs_params = {'penalty': ['l1', 'l2'],\n",
    "                   'solver': ['liblinear', 'saga'],\n",
    "                   'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "                   'class_weight': [None, 'balanced']}\n",
    "\n",
    "scorers = {\n",
    "    'kappa': make_scorer(cohen_kappa_score),\n",
    "    'f1': make_scorer(f1_score)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:    2.6s\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed:    8.6s\n",
      "[Parallel(n_jobs=-1)]: Done 240 out of 240 | elapsed:   15.4s finished\n"
     ]
    }
   ],
   "source": [
    "gs_logit = GridSearchCV(logit, logit_gs_params, scoring=scorers, refit='f1', cv=5, verbose=2, n_jobs=-1)\n",
    "gs_logit = gs_logit.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4122191012059292"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_logit.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 10, 'class_weight': 'balanced', 'penalty': 'l2', 'solver': 'liblinear'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_logit.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=10, class_weight='balanced', dual=False,\n",
       "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
       "                   max_iter=100, multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=8, solver='liblinear', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_logit.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the \"best estimator\" logit model\n",
    "logit = LogisticRegression(C=10, class_weight='balanced', dual=False,\n",
    "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
    "                   max_iter=100, multi_class='auto', n_jobs=None, penalty='l2',\n",
    "                   random_state=8, solver='liblinear', tol=0.0001, verbose=0,\n",
    "                   warm_start=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation F1 scores: [0.40718563 0.42480469 0.42436548 0.40897999 0.39575972]\n",
      "Mean CV F1 scores: 0.4122191012059292\n"
     ]
    }
   ],
   "source": [
    "logit.fit(X_train, y_train)\n",
    "y_hat_train_lr = logit.predict(X_train)\n",
    "y_hat_test_lr = logit.predict(X_test)\n",
    "scores = cross_val_score(logit, X_train, y_train, scoring=f1_scorer, cv=kf)\n",
    "\n",
    "print(\"Cross-validation F1 scores:\", scores)\n",
    "print(\"Mean CV F1 scores:\", np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set accuracy score: 0.7964472816847656\n",
      "Test set accuracy score: 0.7966485873876791\n",
      "Test set F1 score on Class 1: 0.41662796098467253\n",
      "Test set Cohen's kappa: 0.31164329851287864\n"
     ]
    }
   ],
   "source": [
    "print(\"Train set accuracy score:\", logit.score(X_train, y_train))\n",
    "print(\"Test set accuracy score:\", logit.score(X_test, y_test))\n",
    "print(\"Test set F1 score on Class 1:\", f1_score(y_test, y_hat_test_lr, average='binary'))\n",
    "print(\"Test set Cohen's kappa:\", cohen_kappa_score(y_test, y_hat_test_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.82      0.88     10961\n",
      "           1       0.31      0.64      0.42      1392\n",
      "\n",
      "    accuracy                           0.80     12353\n",
      "   macro avg       0.63      0.73      0.65     12353\n",
      "weighted avg       0.88      0.80      0.83     12353\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The F1 score on the test set has more than doubled from the initial logistic regression results\n",
    "print(classification_report(y_test, y_hat_test_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entropy criterion could help imbalanced datasets as it computes the logarithm of the probabilities of each class\n",
    "rf = RandomForestClassifier(n_estimators=200, criterion='entropy', max_features='auto', min_samples_split=5, \n",
    "                            class_weight='balanced', random_state=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_gs_params = {'n_estimators': [200, 300, 400, 500, 800],\n",
    "                'max_features': ['log2', 'sqrt'],\n",
    "                'min_samples_split': [2, 5, 10, 20],\n",
    "                'class_weight': [None, 'balanced']}\n",
    "\n",
    "scorers = {\n",
    "    'kappa': make_scorer(cohen_kappa_score),\n",
    "    'f1': make_scorer(f1_score)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 80 candidates, totalling 400 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed:  5.5min\n",
      "[Parallel(n_jobs=-1)]: Done 357 tasks      | elapsed: 12.7min\n",
      "[Parallel(n_jobs=-1)]: Done 400 out of 400 | elapsed: 14.4min finished\n"
     ]
    }
   ],
   "source": [
    "gs_rf = GridSearchCV(rf, rf_gs_params, scoring=scorers, refit='f1', cv=5, verbose=2, n_jobs=-1)\n",
    "gs_rf = gs_rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.465726129853416"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_rf.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'class_weight': 'balanced',\n",
       " 'max_features': 'sqrt',\n",
       " 'min_samples_split': 20,\n",
       " 'n_estimators': 500}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_rf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight='balanced',\n",
       "                       criterion='entropy', max_depth=None, max_features='sqrt',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=20,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=500,\n",
       "                       n_jobs=None, oob_score=False, random_state=8, verbose=0,\n",
       "                       warm_start=False)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_rf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight='balanced',\n",
    "                       criterion='entropy', max_depth=None, max_features='sqrt',\n",
    "                       max_leaf_nodes=None, max_samples=None,\n",
    "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                       min_samples_leaf=1, min_samples_split=20,\n",
    "                       min_weight_fraction_leaf=0.0, n_estimators=500,\n",
    "                       n_jobs=None, oob_score=False, random_state=8, verbose=0,\n",
    "                       warm_start=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation F1 scores: [0.46126341 0.48339483 0.46626506 0.47613293 0.44157442]\n",
      "Mean CV F1 scores: 0.465726129853416\n"
     ]
    }
   ],
   "source": [
    "rf.fit(X_train, y_train)\n",
    "y_hat_train_rf = rf.predict(X_train)\n",
    "y_hat_test_rf = rf.predict(X_test)\n",
    "scores = cross_val_score(rf, X_train, y_train, scoring=f1_scorer, cv=kf)\n",
    "\n",
    "print(\"Cross-validation F1 scores:\", scores)\n",
    "print(\"Mean CV F1 scores:\", np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set accuracy score: 0.874926274156056\n",
      "Test set accuracy score: 0.8490245284546264\n",
      "Test set F1 score on Class 1: 0.47627071047458586\n",
      "Test set Cohen's kappa: 0.392937575560525\n"
     ]
    }
   ],
   "source": [
    "print(\"Train set accuracy score:\", rf.score(X_train, y_train))\n",
    "print(\"Test set accuracy score:\", rf.score(X_test, y_test))\n",
    "print(\"Test set F1 score on Class 1:\", f1_score(y_test, y_hat_test_rf, average='binary'))\n",
    "print(\"Test set Cohen's kappa:\", cohen_kappa_score(y_test, y_hat_test_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.88      0.91     10961\n",
      "           1       0.39      0.61      0.48      1392\n",
      "\n",
      "    accuracy                           0.85     12353\n",
      "   macro avg       0.67      0.74      0.69     12353\n",
      "weighted avg       0.88      0.85      0.86     12353\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The Random Forest's F1 score on the test set is much better than logistic regression above, \n",
    "# and overall accuracy score is also higher\n",
    "print(classification_report(y_test, y_hat_test_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to fix max_iter at a finite value to reduce the time it takes for the gridsearch to complete.\n",
    "svc = SVC(C=1, kernel='rbf', gamma=0.01, max_iter=150000, class_weight='balanced', random_state=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_gs_params = {'C': [1, 10, 100, 1000, 10000],\n",
    "                 'kernel': ['poly', 'rbf'],\n",
    "                 'gamma': [0.001, 0.01, 0.1, 1],\n",
    "                 'class_weight': [None, 'balanced']}\n",
    "\n",
    "scorers = {\n",
    "    'kappa': make_scorer(cohen_kappa_score),\n",
    "    'f1': make_scorer(f1_score)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 80 candidates, totalling 400 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:  1.6min\n",
      "C:\\Users\\Tan\\Anaconda3\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed: 18.7min\n",
      "[Parallel(n_jobs=-1)]: Done 357 tasks      | elapsed: 50.3min\n",
      "[Parallel(n_jobs=-1)]: Done 400 out of 400 | elapsed: 59.9min finished\n",
      "C:\\Users\\Tan\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:231: ConvergenceWarning: Solver terminated early (max_iter=150000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "gs_svc = GridSearchCV(svc, svc_gs_params, scoring=scorers, refit='f1', cv=5, verbose=2, n_jobs=-1)\n",
    "gs_svc = gs_svc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.46307030638941216"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_svc.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 1000, 'class_weight': 'balanced', 'gamma': 0.01, 'kernel': 'rbf'}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_svc.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1000, break_ties=False, cache_size=200, class_weight='balanced',\n",
       "    coef0=0.0, decision_function_shape='ovr', degree=3, gamma=0.01,\n",
       "    kernel='rbf', max_iter=150000, probability=False, random_state=8,\n",
       "    shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_svc.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC(C=1000, cache_size=200, class_weight='balanced', coef0=0.0,\n",
    "    decision_function_shape='ovr', degree=3, gamma=0.01, kernel='rbf',\n",
    "    max_iter=150000, probability=False, random_state=8, shrinking=True,\n",
    "    tol=0.001, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tan\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:231: ConvergenceWarning: Solver terminated early (max_iter=150000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Tan\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:231: ConvergenceWarning: Solver terminated early (max_iter=150000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Tan\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:231: ConvergenceWarning: Solver terminated early (max_iter=150000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Tan\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:231: ConvergenceWarning: Solver terminated early (max_iter=150000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Tan\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:231: ConvergenceWarning: Solver terminated early (max_iter=150000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Tan\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:231: ConvergenceWarning: Solver terminated early (max_iter=150000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation F1 scores: [0.45265589 0.49044978 0.4636472  0.46217009 0.44642857]\n",
      "Mean CV F1 scores: 0.46307030638941216\n"
     ]
    }
   ],
   "source": [
    "svc.fit(X_train, y_train)\n",
    "y_hat_train_svc = svc.predict(X_train)\n",
    "y_hat_test_svc = svc.predict(X_test)\n",
    "scores = cross_val_score(svc, X_train, y_train, scoring=f1_scorer, cv=kf)\n",
    "\n",
    "print(\"Cross-validation F1 scores:\", scores)\n",
    "print(\"Mean CV F1 scores:\", np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set accuracy score: 0.8436665163237692\n",
      "Test set accuracy score: 0.8421436088399579\n",
      "Test set F1 score on Class 1: 0.46399120395821886\n",
      "Test set Cohen's kappa: 0.37735849056603776\n"
     ]
    }
   ],
   "source": [
    "print(\"Train set accuracy score:\", svc.score(X_train, y_train))\n",
    "print(\"Test set accuracy score:\", svc.score(X_test, y_test))\n",
    "print(\"Test set F1 score on Class 1:\", f1_score(y_test, y_hat_test_svc, average='binary'))\n",
    "print(\"Test set Cohen's kappa:\", cohen_kappa_score(y_test, y_hat_test_svc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.87      0.91     10961\n",
      "           1       0.38      0.61      0.46      1392\n",
      "\n",
      "    accuracy                           0.84     12353\n",
      "   macro avg       0.66      0.74      0.69     12353\n",
      "weighted avg       0.88      0.84      0.86     12353\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_test, y_hat_test_svc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest had the best F1 score and Cohen's kappa on the test set of the three models in the Gridsearch. Random Forest and SVC both outperformed logistic regression handily, possibly due to non-linear relationships in the dataset. The CV F1 scores on the train set were rather similar between the RF and SVC models.\n",
    "\n",
    "In terms of the F1 score on the test set, the rankings from lowest to highest are: Logit (0.42), SVC (0.46) & Random Forest (0.48).\n",
    "\n",
    "In terms of the Kappa score on the test set, the rankings from lowest to highest are: Logit (0.31), SVC(0.38) & Random Forest (0.39).\n",
    "\n",
    "Time now to resample the training data to balance up the minority class to see that helps with improving the various models'predictive power on the test set. I will use the combination SMOTE-Tomek links resampling technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resampling the Train Set & Gridsearch Again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Oversampling the minority positive class with SMOTE, and undersampling the majority negative class Tomek links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "smto = SMOTETomek(tomek=TomekLinks(sampling_strategy='majority'), random_state=8)\n",
    "X_train_st, y_train_st = smto.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28823, 26)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50482, 26)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_st.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28823,)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50482,)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_st.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression with resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit = LogisticRegression(penalty='l2', C=1, solver='liblinear', class_weight='balanced', random_state=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_params = {'class__penalty': ['l1', 'l2'],\n",
    "                'class__solver': ['liblinear', 'saga'],\n",
    "                'class__C': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "\n",
    "pipeline = Pipeline([('sampling', smto), ('class', logit)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=-1)]: Done 120 out of 120 | elapsed:  8.7min finished\n"
     ]
    }
   ],
   "source": [
    "resample_logit = GridSearchCV(pipeline, logit_params, scoring=f1_scorer, cv=5, verbose=2, n_jobs=-1)\n",
    "resample_logit = resample_logit.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.405996978373535"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resample_logit.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('sampling',\n",
       "                 SMOTETomek(n_jobs=None, random_state=8,\n",
       "                            sampling_strategy='auto', smote=None,\n",
       "                            tomek=TomekLinks(n_jobs=None,\n",
       "                                             sampling_strategy='majority'))),\n",
       "                ('class',\n",
       "                 LogisticRegression(C=100, class_weight='balanced', dual=False,\n",
       "                                    fit_intercept=True, intercept_scaling=1,\n",
       "                                    l1_ratio=None, max_iter=100,\n",
       "                                    multi_class='auto', n_jobs=None,\n",
       "                                    penalty='l1', random_state=8,\n",
       "                                    solver='liblinear', tol=0.0001, verbose=0,\n",
       "                                    warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resample_logit.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_re = LogisticRegression(C=100, class_weight='balanced', dual=False, fit_intercept=True, intercept_scaling=1,\n",
    "                           l1_ratio=None, max_iter=100, multi_class='auto', n_jobs=None, penalty='l1', random_state=8,\n",
    "                           solver='liblinear', tol=0.0001, verbose=0, warm_start=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation F1 scores: [0.70755423 0.71027837 0.70377948 0.70883641 0.7050993 ]\n",
      "Mean CV F1 scores: 0.707109558301049\n"
     ]
    }
   ],
   "source": [
    "logit_re.fit(X_train_st, y_train_st)\n",
    "y_hat_train_lr = logit_re.predict(X_train_st)\n",
    "y_hat_test_lr = logit_re.predict(X_test)\n",
    "scores = cross_val_score(logit_re, X_train_st, y_train_st, scoring=f1_scorer, cv=kf)\n",
    "\n",
    "print(\"Cross-validation F1 scores:\", scores)\n",
    "print(\"Mean CV F1 scores:\", np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set accuracy score: 0.7286161404064815\n",
      "Test set accuracy score: 0.7900914757548774\n",
      "Test set F1 score on Class 1: 0.4102797361837617\n",
      "Test set Cohen's kappa: 0.3029124060143258\n"
     ]
    }
   ],
   "source": [
    "print(\"Train set accuracy score:\", logit_re.score(X_train_st, y_train_st))\n",
    "print(\"Test set accuracy score:\", logit_re.score(X_test, y_test))\n",
    "print(\"Test set F1 score on Class 1:\", f1_score(y_test, y_hat_test_lr, average='binary'))\n",
    "print(\"Test set Cohen's kappa:\", cohen_kappa_score(y_test, y_hat_test_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Much higher CV F1 scores on the train set (~0.71 vs ~0.41) after the resampling, but the results from the test set are slightly worse than the original scores from Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest with resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=200, criterion='entropy', max_features='auto', min_samples_split=5, \n",
    "                            class_weight='balanced', random_state=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_params = {'class__n_estimators': [200, 300, 400, 500, 800],\n",
    "             'class__max_features': ['log2', 'sqrt'],\n",
    "             'class__min_samples_split': [2, 5, 10, 20]}\n",
    "\n",
    "pipeline = Pipeline([('sampling', smto), ('class', rf)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:  6.2min\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed: 28.7min\n",
      "[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed: 37.7min finished\n"
     ]
    }
   ],
   "source": [
    "resample_rf = GridSearchCV(pipeline, rf_params, scoring=f1_scorer, cv=5, verbose=2, n_jobs=-1)\n",
    "resample_rf = resample_rf.fit(X_train_st, y_train_st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8991214669347718"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resample_rf.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('sampling',\n",
       "                 SMOTETomek(n_jobs=None, random_state=8,\n",
       "                            sampling_strategy='auto', smote=None,\n",
       "                            tomek=TomekLinks(n_jobs=None,\n",
       "                                             sampling_strategy='majority'))),\n",
       "                ('class',\n",
       "                 RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,\n",
       "                                        class_weight='balanced',\n",
       "                                        criterion='entropy', max_depth=None,\n",
       "                                        max_features='sqrt',\n",
       "                                        max_leaf_nodes=None, max_samples=None,\n",
       "                                        min_impurity_decrease=0.0,\n",
       "                                        min_impurity_split=None,\n",
       "                                        min_samples_leaf=1, min_samples_split=5,\n",
       "                                        min_weight_fraction_leaf=0.0,\n",
       "                                        n_estimators=800, n_jobs=None,\n",
       "                                        oob_score=False, random_state=8,\n",
       "                                        verbose=0, warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resample_rf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_re = RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,\n",
    "                                        class_weight='balanced',\n",
    "                                        criterion='entropy', max_depth=None,\n",
    "                                        max_features='sqrt',\n",
    "                                        max_leaf_nodes=None, max_samples=None,\n",
    "                                        min_impurity_decrease=0.0,\n",
    "                                        min_impurity_split=None,\n",
    "                                        min_samples_leaf=1, min_samples_split=5,\n",
    "                                        min_weight_fraction_leaf=0.0,\n",
    "                                        n_estimators=800, n_jobs=None,\n",
    "                                        oob_score=False, random_state=8,\n",
    "                                        verbose=0, warm_start=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation F1 scores: [0.79977949 0.92910807 0.9237911  0.92780723 0.92178609]\n",
      "Mean CV F1 scores: 0.900454397817698\n"
     ]
    }
   ],
   "source": [
    "rf_re.fit(X_train_st, y_train_st)\n",
    "y_hat_train_rf = rf_re.predict(X_train_st)\n",
    "y_hat_test_rf = rf_re.predict(X_test)\n",
    "scores = cross_val_score(rf_re, X_train_st, y_train_st, scoring=f1_scorer, cv=kf)\n",
    "\n",
    "print(\"Cross-validation F1 scores:\", scores)\n",
    "print(\"Mean CV F1 scores:\", np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set accuracy score: 0.9677904995840101\n",
      "Test set accuracy score: 0.851210232332227\n",
      "Test set F1 score on Class 1: 0.40169270833333337\n",
      "Test set Cohen's kappa: 0.3175855247718634\n"
     ]
    }
   ],
   "source": [
    "print(\"Train set accuracy score:\", rf_re.score(X_train_st, y_train_st))\n",
    "print(\"Test set accuracy score:\", rf_re.score(X_test, y_test))\n",
    "print(\"Test set F1 score on Class 1:\", f1_score(y_test, y_hat_test_rf, average='binary'))\n",
    "print(\"Test set Cohen's kappa:\", cohen_kappa_score(y_test, y_hat_test_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Similarly improved performance on the train set following resampling, but worse scores on the test set with Random Forest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine Classifier with resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC(C=1, kernel='rbf', gamma=0.01, max_iter=150000, class_weight='balanced', random_state=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_params = {'class__C': [1, 10, 100, 1000, 10000],\n",
    "              'class__kernel': ['poly', 'rbf'],\n",
    "              'class__gamma': [0.001, 0.01, 0.1, 1]}\n",
    "\n",
    "pipeline = Pipeline([('sampling', smto), ('class', svc)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed: 20.2min\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed: 95.6min\n",
      "[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed: 128.2min finished\n",
      "C:\\Users\\Tan\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:231: ConvergenceWarning: Solver terminated early (max_iter=150000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "resample_svc = GridSearchCV(pipeline, svc_params, scoring=f1_scorer, cv=5, verbose=2, n_jobs=-1)\n",
    "resample_svc = resample_svc.fit(X_train_st, y_train_st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8089049802758371"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resample_svc.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('sampling',\n",
       "                 SMOTETomek(n_jobs=None, random_state=8,\n",
       "                            sampling_strategy='auto', smote=None,\n",
       "                            tomek=TomekLinks(n_jobs=None,\n",
       "                                             sampling_strategy='majority'))),\n",
       "                ('class',\n",
       "                 SVC(C=10000, break_ties=False, cache_size=200,\n",
       "                     class_weight='balanced', coef0=0.0,\n",
       "                     decision_function_shape='ovr', degree=3, gamma=1,\n",
       "                     kernel='rbf', max_iter=150000, probability=False,\n",
       "                     random_state=8, shrinking=True, tol=0.001,\n",
       "                     verbose=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resample_svc.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_re = SVC(C=10000, break_ties=False, cache_size=200,\n",
    "                     class_weight='balanced', coef0=0.0,\n",
    "                     decision_function_shape='ovr', degree=3, gamma=1,\n",
    "                     kernel='rbf', max_iter=150000, probability=False,\n",
    "                     random_state=8, shrinking=True, tol=0.001,\n",
    "                     verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tan\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:231: ConvergenceWarning: Solver terminated early (max_iter=150000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Tan\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:231: ConvergenceWarning: Solver terminated early (max_iter=150000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Tan\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:231: ConvergenceWarning: Solver terminated early (max_iter=150000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Tan\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:231: ConvergenceWarning: Solver terminated early (max_iter=150000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Tan\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:231: ConvergenceWarning: Solver terminated early (max_iter=150000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Tan\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:231: ConvergenceWarning: Solver terminated early (max_iter=150000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation F1 scores: [0.75232131 0.82730515 0.8184138  0.82168217 0.82345913]\n",
      "Mean CV F1 scores: 0.8086363104392564\n"
     ]
    }
   ],
   "source": [
    "svc_re.fit(X_train_st, y_train_st)\n",
    "y_hat_train_svc = svc_re.predict(X_train_st)\n",
    "y_hat_test_svc = svc_re.predict(X_test)\n",
    "scores = cross_val_score(svc_re, X_train_st, y_train_st, scoring=f1_scorer, cv=kf)\n",
    "\n",
    "print(\"Cross-validation F1 scores:\", scores)\n",
    "print(\"Mean CV F1 scores:\", np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set accuracy score: 0.8369121667128878\n",
      "Test set accuracy score: 0.7936533635554116\n",
      "Test set F1 score on Class 1: 0.3048813744205072\n",
      "Test set Cohen's kappa: 0.19189227747848947\n"
     ]
    }
   ],
   "source": [
    "print(\"Train set accuracy score:\", svc_re.score(X_train_st, y_train_st))\n",
    "print(\"Test set accuracy score:\", svc_re.score(X_test, y_test))\n",
    "print(\"Test set F1 score on Class 1:\", f1_score(y_test, y_hat_test_svc, average='binary'))\n",
    "print(\"Test set Cohen's kappa:\", cohen_kappa_score(y_test, y_hat_test_svc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rather poor outcome with SVC on the test set. It has the worst test set results post-resampling in terms of the F1 score and Cohen's kappa among the three models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of the models' post-resampling classification reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.81      0.87     10961\n",
      "           1       0.30      0.65      0.41      1392\n",
      "\n",
      "    accuracy                           0.79     12353\n",
      "   macro avg       0.62      0.73      0.64     12353\n",
      "weighted avg       0.87      0.79      0.82     12353\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Logistic regression\n",
    "print(classification_report(y_test, y_hat_test_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.90      0.92     10961\n",
      "           1       0.37      0.44      0.40      1392\n",
      "\n",
      "    accuracy                           0.85     12353\n",
      "   macro avg       0.65      0.67      0.66     12353\n",
      "weighted avg       0.86      0.85      0.86     12353\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Random forest\n",
    "print(classification_report(y_test, y_hat_test_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.84      0.88     10961\n",
      "           1       0.25      0.40      0.30      1392\n",
      "\n",
      "    accuracy                           0.79     12353\n",
      "   macro avg       0.58      0.62      0.59     12353\n",
      "weighted avg       0.84      0.79      0.81     12353\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SVC\n",
    "print(classification_report(y_test, y_hat_test_svc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All three models' \"best estimators\" on the train set produced much higher F1 scores following the resampling procedure. The Random Forest model achieved the highest cross-validation F1 scores in particular. \n",
    "\n",
    "#### However, the use of the combination SMOTE and Tomek links algorithms failed to boost the predictions from all three models on the test data. In short, the resampling procedure did not manage to improve the performance of the models on the test set, so the improvements seen in the training data were not generalisable beyond it.\n",
    "\n",
    "#### I will use Random Forest as the algorithm of choice for the All-variables predictive model (without resampling) because it was the top model to emerge from the Gridsearch procedures above. Furthermore, one could explore how much each explanatory variable contributed to the overall model prediction with Random Forest, unlike the black-box SVC . So if model interpretability and transparency are important, these are additional reasons to pick RF over SVC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the poor test results, I will not be using resampling for the All-variables predictive model of choice. Other oversampling and undersampling techniques could be analysed to see if they might result in more generalisable improvements to the predictive models. But a lot more time would be needed to do it comprehensively."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
